# 内核是如何与用户进程协作的

## 内核是如何与用户进程协作的

### 相关实际问题

这次的问题没有那么底层了, 主要是关于阻塞和网络性能的问题

### socket的直接创建

socket在内核中的结构

```
从开发者的角度, 可以通过一个socket函数来创建, 返回值是一个整数,而内核内部会创建一系列的对象.

当软中断收到数据包时会通过调用sk_data_ready函数指针(实际上被设置成socke_def_readable())来映醒生在sock上等待的进程.
```

### 内核和用户进程协作的阻塞方式

同步阻塞方式(JAVA BIO)

```
用户发起创建socket指令, 然后切换到内核态完成内核 对象的初台化.接下来, linux在数据包的接收上, 是硬中断和ksoftiqd再进行处理. 当ksoftirqd线程处理完以后, 再通知相关的用户进程. 
```

#### 等待接收消息

recv 函数依赖底层实现, 最后在socket对象的收妆队列中查看是否有数据, 没有的话就会自己加到对应的等待队列里, 最后让出CPU.

多个调用的结果就是, 当没有数据到达时, 会调用sk\_wait\_event让出CPU, 进程将进入睡眠状态, 导致一次进程上下文的开销.

#### 软中断模块

TODO 今天是真的没时间看了, 代码好长, 改天再看

#### 同步阻塞总结

分成两部分:\
1, 自己代码所在的进程, 调用socket()函数进到内核态创建必要内核对象. recv()函数在进八内核态以后负责查看接收队列, 以及在没有数据可怍理的时候把当前进程阻塞掉, 让出CPU.\
2, 硬中断, 软中断上下文. 在这些组件中, 将包处理完合会放到socket的接收队列中.然后根据socket内核对象找到其等待队列中因为等待而被阻塞掉的进程,把它唤醒.

单路不复用 : 进程要等到数据收到之后才会继续, 对客户端来说太卡, 对于服务端来说浪费进程.

### epoll

IO多路复用机制, 特指对进程的复用.\
linux多路复用机制有select poll epoll 三种\
三个相关函数

```
epoll_create 创建一个epoll对象
epoll_ctl 向对象添加要管理的连接
epoll_wait 等待其管理的连接上的io事件
```

#### epoll 内核对象的创建

几个比较重要的结构体

wq : 等待队列链表

rbr : 一个红黑树管理所有socket连接

rdllist : 就绪的描述符链表

#### 为epoll 添加 socket

就是前面提到的那个红黑树,初始化一个节点, 再把一个等待事件加到等待队列中.

#### epoll\_wait 等待接收

判断就绪队列上有没有事件就绪\
定义等待事件并关联当前进程\
添加到等待队列\
让出CPU主动进入睡眠状态

#### 数据来了...呢

将数据接收到任务队列\
查找就绪回调函数\
执行socke就绪回调函数\
执行epoll就绪通知

#### 小结

P79 有个图, 感觉这个图还是挺不错的

epoll相关的函数里内核运行环境分为两部分

* 用户进程内核态, 调用epoll\_wait等 函数时会进入内核态执行, 负责查看接收队列 , 以及把当前进程阻塞掉, 让出CPU.
* 软中断上下文

重点

```
实践中, 只要活足够多, epoll_wait根不不会让进程阻塞. 
```

### 总结

回答一堆之前的问题

阻塞到底是怎么一回事

```
进程因为等待某个事件而主动让出CPU挂起的操作
```

同步阻塞IO都需要哪些开销

```
进程上下文切换的开销
多个进程多并发时内存的开销
```

为什么epoll可以提高网络性能

```
极大程度地减少了无用的进程上下文切换
```

epoll 也是阻塞的

```
阻塞不会降低性能,过多的阻塞才会
```

redis性能好的原因

```
主要业务是在本机内存上的数据结构读写, 没有网络和硬盘IO
```
